#!/usr/bin/env python3
"""
LiteLLM Maven Vulnerability Scanner Integration

This script integrates a Maven vulnerability scanner MCP server with LiteLLM
using Gemini 2.5 Flash model for interactive vulnerability analysis.
"""

import asyncio
import json
import os
import re
import sys
import traceback
from pathlib import Path
from typing import Any, Dict, List

import litellm
from dotenv import load_dotenv
from litellm import experimental_mcp_client
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

load_dotenv()


# ============================================================================
# CONFIGURATION - Change these settings to switch between models
# ============================================================================
LLM_PROVIDER = "OPENAI"  # Options: "GEMINI" or "OPENAI"

# Model configurations
GEMINI_MODEL = "gemini/gemini-2.0-flash-exp"
OPENAI_MODEL = "gpt-4o"  # Options: gpt-4o, gpt-4-turbo, gpt-3.5-turbo

# Common settings
TEMPERATURE = 0.1
SEPARATOR = "=" * 80

# ============================================================================


def get_model_config():
    """Get the current model configuration based on LLM_PROVIDER."""
    if LLM_PROVIDER == "GEMINI":
        return {
            "model": GEMINI_MODEL,
            "api_key": os.getenv("GEMINI_API_KEY"),
            "provider_name": "Gemini 2.5 Flash"
        }
    elif LLM_PROVIDER == "OPENAI":
        return {
            "model": OPENAI_MODEL,
            "api_key": os.getenv("OPENAI_API_KEY"),
            "provider_name": f"OpenAI {OPENAI_MODEL}"
        }
    else:
        raise ValueError(f"Invalid LLM_PROVIDER: {LLM_PROVIDER}. Use 'GEMINI' or 'OPENAI'")


def read_pom_file(file_path: str) -> str:
    """Read pom.xml file from the given path."""
    try:
        path = Path(file_path).expanduser()
        if not path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        raise Exception(f"Error reading file: {e}") from e


def print_startup_banner():
    """Print startup banner."""
    config = get_model_config()
    print(f"\n{SEPARATOR}")
    print("[MCP] Maven Vulnerability Scanner MCP Integration")
    print(f"[LLM] Using: {config['provider_name']}")
    print(SEPARATOR)


def print_chat_instructions():
    """Print chat instructions."""
    config = get_model_config()
    print(f"\n{'=' * 50}")
    print(f"Chat with {config['provider_name']} + Maven Vulnerability Scanner")
    print("=" * 50)
    print("This is a normal chat with access to vulnerability scanning.")
    print("To scan a pom file, just paste the file path or ask me to scan.")
    print("Type 'quit' to exit")
    print("=" * 50)


def get_system_prompt() -> str:
    """Get the system prompt for the LLM."""
    return """You are a helpful AI assistant with access to a Maven vulnerability scanner tool.

You can have normal conversations about any topic. When the user wants to scan a pom.xml file for vulnerabilities:
- ALWAYS ask for the file path, never ask for file content to be pasted
- When the user mentions a file path or asks to scan a file, the system will automatically load it
- The pom.xml content will be included in the message when a file is loaded
- Use the scan_pom_vulnerabilities tool when pom.xml content is provided in the message
- Provide clear, helpful analysis of the results

That's it - no restrictions, just be helpful!"""


def is_file_path(user_input: str) -> bool:
    """Check if user input looks like a file path."""
    stripped = user_input.strip()
    return (
        stripped.startswith("/") or
        stripped.startswith("~") or
        stripped.startswith("./") or
        stripped.startswith("../") or
        stripped.lower().startswith("scan ")
    )


def extract_file_path(user_input: str) -> str:
    """Extract file path from user input."""
    if user_input.lower().startswith("scan "):
        return user_input[5:].strip()
    return user_input.strip()


def find_file_paths_in_text(text: str) -> list[str]:
    """Find potential file paths in text."""
    patterns = [
        r'(/[^\s]+\.xml)',      # Absolute path ending in .xml
        r'(~/[^\s]+\.xml)',     # Home path ending in .xml
        r'(\./[^\s]+\.xml)',    # Relative path ending in .xml
        r'(\.\./[^\s]+\.xml)',  # Parent path ending in .xml
    ]

    paths = []
    for pattern in patterns:
        matches = re.findall(pattern, text)
        paths.extend(matches)

    return paths


def parse_tool_call(tool_call) -> tuple[str, dict]:
    """Parse tool call to extract name and arguments."""
    name = None
    args_raw = None

    if hasattr(tool_call, "function"):
        name = tool_call.function.name
        args_raw = tool_call.function.arguments
    else:
        fn = tool_call.get("function", {}) if isinstance(tool_call, dict) else {}
        name = fn.get("name")
        args_raw = fn.get("arguments")

    args = json.loads(args_raw) if isinstance(args_raw, str) else (args_raw or {})
    return name, args


def get_tool_call_id(tool_call) -> str:
    """Extract tool call ID."""
    if hasattr(tool_call, "id"):
        return tool_call.id
    if isinstance(tool_call, dict):
        return tool_call.get("id")
    return None


async def process_tool_calls(
    session: ClientSession,
    tool_calls: list,
    messages: List[Dict[str, Any]]
) -> None:
    """Process tool calls from the LLM."""
    config = get_model_config()
    print(f"\n[MCP] ⚡ {config['provider_name']} requested {len(tool_calls)} tool call(s)")

    messages.append({
        "role": "assistant",
        "content": None,
        "tool_calls": [
            tc.model_dump() if hasattr(tc, "model_dump") else tc
            for tc in tool_calls
        ],
    })

    for idx, tc in enumerate(tool_calls, 1):
        try:
            name, args = parse_tool_call(tc)
        except Exception:
            print(f"[MCP] ✗ Failed parsing tool call {idx} arguments")
            traceback.print_exc()
            continue

        print(f"[MCP] → Calling MCP server tool: {name}")
        pom_size = len(args.get('pom_content', ''))
        print(f"[MCP]   Arguments: pom_content={pom_size} characters")

        try:
            print("[MCP]   Executing vulnerability scan on MCP server...")
            print("[MCP]   Scanning dependencies (this may take a moment)...")
            result = await session.call_tool(name, args)
            result_text = (
                result.content[0].text
                if getattr(result, "content", None)
                else str(result)
            )

            result_preview = (
                result_text[:150] + "..."
                if len(result_text) > 150
                else result_text
            )
            print(f"[MCP] ✓ MCP server returned: {len(result_text)} characters")
            print(f"[MCP]   Preview: {result_preview}")
        except Exception as e:
            print(f"[MCP] ✗ MCP tool '{name}' failed: {e}")
            traceback.print_exc()
            result_text = ""

        messages.append({
            "role": "tool",
            "tool_call_id": get_tool_call_id(tc),
            "name": name,
            "content": result_text,
        })


async def chat_loop(session: ClientSession, llm_tools: list) -> None:
    """Main chat loop."""
    print_chat_instructions()

    messages: List[Dict[str, Any]] = []
    messages.append({"role": "system", "content": get_system_prompt()})

    while True:
        try:
            user_input = input("\nYou: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nGoodbye!")
            break

        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break

        if not user_input:
            continue

        # Check if entire input is a file path (original behavior)
        file_loaded = False
        if is_file_path(user_input):
            file_path = extract_file_path(user_input)
            try:
                pom_content = read_pom_file(file_path)
                user_input = f"I have a pom.xml file to scan:\n\n{pom_content}"
                print(f"[File loaded: {file_path}]")
                file_loaded = True
            except Exception:
                pass

        # If not a direct file path, look for file paths mentioned in the text
        if not file_loaded:
            found_paths = find_file_paths_in_text(user_input)
            if found_paths:
                # Try to load the first valid file found
                for path in found_paths:
                    try:
                        pom_content = read_pom_file(path)
                        # Append the file content to the user's message
                        user_input = f"{user_input}\n\n[File content from {path}]:\n{pom_content}"
                        print(f"[File loaded: {path}]")
                        file_loaded = True
                        break
                    except Exception:
                        continue

        messages.append({"role": "user", "content": user_input})

        try:
            config = get_model_config()
            print(f"[LLM] Sending request to {config['provider_name']} with MCP tools...")
            response = await litellm.acompletion(
                model=config["model"],
                messages=messages,
                tools=llm_tools,
                tool_choice="auto",
                temperature=TEMPERATURE,
                api_key=config["api_key"],
            )

            while True:
                choice = response.choices[0]
                msg = choice.message
                tool_calls = getattr(msg, "tool_calls", None)

                if not tool_calls:
                    content = getattr(msg, "content", "")
                    print(f"\nAssistant: {content}")
                    messages.append({"role": "assistant", "content": content})
                    break

                await process_tool_calls(session, tool_calls, messages)

                config = get_model_config()
                print(f"[LLM] Sending MCP results back to {config['provider_name']} for analysis...")
                response = await litellm.acompletion(
                    model=config["model"],
                    messages=messages,
                    tools=llm_tools,
                    tool_choice="auto",
                    temperature=TEMPERATURE,
                    api_key=config["api_key"],
                )
        except Exception:
            print("[ERROR] LiteLLM tool-call flow failed")
            traceback.print_exc()


async def chat_with_maven_scanner() -> None:
    """Main function to start the chat with MCP integration."""
    print_startup_banner()

    try:
        server_params = StdioServerParameters(
            command=sys.executable,
            args=["server.py"],
            env=None,
        )

        print("[MCP] Launching MCP server: server.py")

        async with stdio_client(server_params) as (read, write):
            print("[MCP] ✓ MCP server process started")

            async with ClientSession(read, write) as session:
                print("[MCP] ✓ Establishing MCP session connection...")

                await session.initialize()
                print("[MCP] ✓ MCP session initialized successfully")
                print("[MCP] Note: Progress tracking available during scans")

                try:
                    print("[MCP] Loading tools from MCP server...")
                    llm_tools = await experimental_mcp_client.load_mcp_tools(
                        session=session,
                        format="openai"
                    )
                    print(f"[MCP] ✓ Successfully loaded {len(llm_tools)} tool(s)")

                    for i, tool in enumerate(llm_tools, 1):
                        tool_name = tool.get('function', {}).get('name', 'unknown')
                        print(f"[MCP]   Tool {i}: {tool_name}")
                except Exception:
                    print("[MCP] ✗ Failed to load MCP tools")
                    traceback.print_exc()
                    return

                if not sys.stdin.isatty():
                    print("[WARN] Non-interactive terminal detected")
                    return

                await chat_loop(session, llm_tools)

    except Exception:
        print("[ERROR] chat_with_maven_scanner failed")
        traceback.print_exc()


def main() -> None:
    """Entry point."""
    config = get_model_config()

    if not config["api_key"]:
        if LLM_PROVIDER == "GEMINI":
            print("Please set your GEMINI_API_KEY in the .env file")
            print("   Edit the .env file and add: GEMINI_API_KEY=your-api-key-here")
            print("   Get your API key from: https://aistudio.google.com/app/apikey")
        elif LLM_PROVIDER == "OPENAI":
            print("Please set your OPENAI_API_KEY in the .env file")
            print("   Edit the .env file and add: OPENAI_API_KEY=your-api-key-here")
            print("   Get your API key from: https://platform.openai.com/api-keys")
        sys.exit(1)

    asyncio.run(chat_with_maven_scanner())


if __name__ == "__main__":
    main()
